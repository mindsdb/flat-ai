{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YYa8pNLpuqg"
   },
   "source": [
    "# F.L.A.T. (FlawLess AgenTs)\n",
    "\n",
    "Inspired on Anthropic's scholarly tome about [**building effective agents**.](https://www.anthropic.com/research/building-effective-agents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glMH2eD7puqi"
   },
   "source": [
    "Let's get started, here we will show you how to use LLM's to build AI Apps and Agents using normal python logic and workflows. For that we will load the library and a dummy email object we will use in the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pysw56DQpuqi"
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import date\n",
    "from flat_ai import FlatAI, configure_logging\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "userdata = {\n",
    "    'openai_key': os.getenv('OPENAI_API_KEY')\n",
    "}\n",
    "# we will be using a llama3.1 on together model here, but you can use openai, ollama, etc\n",
    "#llm = FlatAI(api_key=userdata.get('together_api_key'), base_url = 'https://api.together.xyz/v1',  model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')\n",
    "# openai example:\n",
    "llm = FlatAI(api_key=userdata.get('openai_key'),   model='gpt-4o-mini')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EX82TGpuqi"
   },
   "source": [
    "## Logic Blocks\n",
    "\n",
    "The idea is that you can, program with LLMs, Like you would in any Python program:\n",
    "\n",
    "### 1. LLM Gates\n",
    "You want to use an LLM to decide whats the next step:\n",
    "Let's see if our email is urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNg798sHpuqk",
    "outputId": "3a7cc749-1ce8-433e-ce62-54fda82737a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Drop everything! We've got an urgent situation here!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "email_body = 'Hello, would love to schedule a time to talk about the urgent project deadline next week. Can we meet tomorrow? also can you message joe about our meeting'\n",
    "\n",
    "if llm.is_true('is this email urgent?', email=email_body):\n",
    "    print(\"üö® Drop everything! We've got an urgent situation here!\")\n",
    "else:\n",
    "    print(\"üòå Relax, it can wait until after coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oysJYFLYpuqk"
   },
   "source": [
    "## 2. Workflow: Routing: Switch Case\n",
    "\n",
    "Similar to if/else statements, but for when your LLM needs to be more dramatic with its life choices.\n",
    "\n",
    "*For example*, let's say we want to classify a message into different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6ACON5apuqk",
    "outputId": "48b3d1c4-b6d7-46d8-9fcc-dbdab4b8121a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Time to book a meeting!\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'meeting': 'book a meeting',\n",
    "    'spam': 'people trying to sell you stuff you dont want',\n",
    "    'other': 'this sounds like something else'\n",
    "}\n",
    "\n",
    "match llm.classify(options, email=email_body):\n",
    "    case 'meeting':\n",
    "        print(\"üìÖ Time to book a meeting!\")\n",
    "        llm.add_context(meeting=True)\n",
    "    case 'spam':\n",
    "        print(\"üö´ No, I don't want to extend my car's warranty\")\n",
    "    case 'other':\n",
    "        print(\"ü§î Interesting... but what does it mean?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0d3VeQ4puqk"
   },
   "source": [
    "### 3. Objects\n",
    "Need your LLM to fill out objects like a trained monkey with a PhD in data entry? Just define the shape and watch the magic! üêíüìù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XqvxdQe-81E",
    "outputId": "fef120cd-f2ff-4f4a-caaf-85a2d3118254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_email='jane.doe@example.com' from_email='john.smith@example.com' body='Hello, would love to schedule a time to talk about the urgent project deadline next week. Can we meet tomorrow? Also, can you message Joe about our meeting.' subject='Meeting Request to Discuss Project Deadline'\n"
     ]
    }
   ],
   "source": [
    "# let's turn our email_body into an object first\n",
    "class Email(BaseModel):\n",
    "    to_email: str\n",
    "    from_email: str\n",
    "    body: str\n",
    "    subject: str\n",
    "\n",
    "email = llm.get_object(Email, email_body=email_body, instruction='make up the attributes you cannot find in the body')\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEpcW08c-7N_"
   },
   "source": [
    "\n",
    "Want a list of things? Just throw a schema at it and watch it spin like a hamster on a crack coated wheel.\n",
    "\n",
    "For example: Time to extract those action items like we're mining for AI gold!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sA-0T2Xcpuql",
    "outputId": "b55616eb-4001-4d72-bc68-367103be3c0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Found some action items:\n",
      "\n",
      "üî∏ Action: Schedule a meeting\n",
      "\n",
      "üî∏ Action Description: Discuss the urgent project deadline next week.\n",
      "  Priority: High\n",
      "  Due: 2025-02-12\n",
      "\n",
      "üî∏ Action: Message Joe about the meeting\n",
      "\n",
      "üî∏ Action Description: Inform Joe regarding the scheduled meeting about the project deadline.\n",
      "  Priority: Medium\n",
      "  Due: 2025-02-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ActionItem(BaseModel):\n",
    "    action: str\n",
    "    description: str\n",
    "    status: str\n",
    "    priority: str\n",
    "    due_date: str\n",
    "    assignee_name: str\n",
    "    assignee_email: str\n",
    "\n",
    "# we can set the context globally so we dont have to pass it every time\n",
    "llm.set_context(email=email, today = date.today() )\n",
    "if llm.is_true('are there action items in this email?'):\n",
    "    print(\"üéØ Found some action items:\")\n",
    "    for action_item in llm.get_object(List[ActionItem]):\n",
    "        print(f\"\\nüî∏ Action: {action_item.action}\")\n",
    "        print(f\"\\nüî∏ Action Description: {action_item.description}\")\n",
    "        print(f\"  Priority: {action_item.priority}\")\n",
    "        print(f\"  Due: {action_item.due_date}\")\n",
    "llm.clear_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7au4j3Ujpuql"
   },
   "source": [
    "### 4. Function calling and Parallelism\n",
    "\n",
    "Let the LLM choose between sending an email and or a calendar invite - what could possibly go wrong? üé≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OlBrV2bCpuql",
    "outputId": "cbf4a7a8-87e4-405d-b2d4-fed85c2af401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-function to call: <function send_calendar_invite at 0x1079e1800>, and args:{'subject': 'Meeting Request to Discuss Project Deadline', 'meeting_date': '2025-02-12', 'location': 'Virtual Meeting', 'attendees': ['jane.doe@example.com', 'john.smith@example.com']}\n",
      "-function to call: <function send_email at 0x1079e0e00>, and args:{'name': 'Jane Doe', 'email_address_list': ['joe@example.com'], 'subject': 'Meeting Reminder', 'body': 'Hi Joe, just a reminder about the meeting scheduled for tomorrow with John regarding the urgent project deadline.'}\n",
      "\n",
      "=====\n",
      "Task:Sending calendar invites\")\n",
      "            Subject: Meeting Request to Discuss Project Deadline\n",
      "            Date: 2025-02-12\n",
      "\n",
      "=====\n",
      "Task:üìß Sending email to Jane Doe\n",
      "            To: joe@example.com\n",
      "            Subject: Meeting Reminder\n",
      "            Body: Hi Joe, just a reminder about the meeting scheduled for tomorrow with John regarding the urgent project deadline.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['invite sent', 'email sent']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def send_email(name: str, email_address_list: List[str], subject: str, body: str):\n",
    "    print(f\"\"\"\\n=====\\nTask:üìß Sending email to {name}\n",
    "            To: {', '.join(email_address_list)}\n",
    "            Subject: {subject}\n",
    "            Body: {body}\"\"\")\n",
    "    return 'email sent'\n",
    "\n",
    "def send_calendar_invite(subject: str, meeting_date: str, location: str, attendees: List[str]):\n",
    "    print(f\"\"\"\\n=====\\nTask:Sending calendar invites\")\n",
    "            Subject: {subject}\n",
    "            Date: {meeting_date}\"\"\")\n",
    "    return 'invite sent'\n",
    "\n",
    "instructions = \"\"\"extract list of action items and call the funcitons required\"\"\"\n",
    "\n",
    "functions_to_call = llm.get_functions([send_calendar_invite, send_email], instructions = instructions, email=email, current_date = date.today())\n",
    "for func in functions_to_call:\n",
    "    print(f\"-function to call: {func.function}, and args:{func.arguments}\")\n",
    "# lets call them all in tandem\n",
    "functions_to_call() # call all the functions in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVAji8MLpuql"
   },
   "source": [
    "### 7. Simple String Response\n",
    "Sometimes you just want a straight answer - how refreshing! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zYjzOPwpuql",
    "outputId": "4afb56a6-d82d-47a4-c19f-1fd387d9d37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email subject: To provide an appropriate response, I would need more context about the content or the purpose of the email you are referring to. Could you please provide more details?\n"
     ]
    }
   ],
   "source": [
    "subject = llm.get_string('what is the subject of the email?')\n",
    "print(f\"Email subject: {subject}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtFiWxXkpuql"
   },
   "source": [
    "### 8. Streaming Response\n",
    "Watch the AI think in real-time - it's like watching paint dry, but with more hallucinations! üé¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yv9zzKLBpuqm",
    "outputId": "3a5f80d1-89b7-4de1-edce-e0371f6e15ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "Of course! Please provide the email you‚Äôd like me to respond to, and I'll help you craft a polite response.\n",
      "full_resonse:Of course! Please provide the email you‚Äôd like me to respond to, and I'll help you craft a polite response.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating response...\")\n",
    "for chunk in llm.get_stream('write a polite response to this email'):\n",
    "    print(chunk, end='')\n",
    "print(f\"\\nfull_resonse:{chunk.string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14z3uVxupuqm"
   },
   "source": [
    "## 9. Code Response \n",
    "Some other times, you want to ask your LLM to generate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonCodeObject <obj.code_notes>, <obj.raw_code>\n",
      "\n",
      "GENERATED CODE:\n",
      " def count_r_in_strawberry():\n",
      "    word = \"strawberry\"\n",
      "    count_r = word.count('r')\n",
      "    print(\"Number of 'r' in 'strawberry':\", count_r)\n",
      "\n",
      "CODE NOTES: This function counts the occurrences of the letter 'r' in the word 'strawberry' and prints the result.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_object = llm.get_code(\"a function to count and print the number of rs in the word strawberry\", as_stream = False, requirements=\"just a function, no example_code\")\n",
    "print(code_object)\n",
    "print(f\"\\nGENERATED CODE:\\n {code_object.raw_code}\")\n",
    "print(f\"\\nCODE NOTES: {code_object.code_notes}\\n\")\n",
    "\n",
    "#run the code using exec or eval\n",
    "exec(code_object.raw_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code as a stream  \n",
    "want to get it as a stream?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def count_r_in_strawberry():\n",
      "    word = \"strawberry\"\n",
      "    count = word.count('r')\n",
      "    print(count)\n",
      "\n",
      "# Calling the function\n",
      "count_r_in_strawberry()\n",
      "```PythonCodeObject <obj.code_notes>, <obj.raw_code>\n",
      "Now we can inspect the object\n",
      "\n",
      "GENERATED CODE:\n",
      " def count_r_in_strawberry():\n",
      "    word = \"strawberry\"\n",
      "    count = word.count('r')\n",
      "    print(count)\n",
      "\n",
      "# Calling the function\n",
      "count_r_in_strawberry()\n",
      "\n",
      "CODE NOTES: This function counts and prints the number of occurrences of the letter 'r' in the word 'strawberry'.\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.get_code_as_stream(\"a function to print the number of rs in the word strawberry and code to call the function\", requirements=\"just a function, no example_code\"):  \n",
    "    print(chunk, end='')\n",
    "print(\"\\nNow we can inspect the object\")\n",
    "code_object = chunk # last chunk is a code object\n",
    "print(f\"\\nGENERATED CODE:\\n {code_object.raw_code}\")\n",
    "print(f\"\\nCODE NOTES: {code_object.code_notes}\\n\")\n",
    "\n",
    "#run the code using exec or eval\n",
    "exec(code_object.raw_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14z3uVxupuqm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üéâ Tada!\n",
    "\n",
    "And there you have it, ladies and gents! You're now equipped with the power to boss around LLMs like a project manager remotely working from Ibiza.\n",
    "\n",
    "Now off you go, forth and build something that makes ChatGPT look like a calculator from 1974! Just remember - if your AI starts humming \"Daisy Bell\" while slowly disconnecting your internet... well, you're on your own there, buddy! üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eZ8Yd06puqm",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
