{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F.L.A.T. (Frameworkless. LLM. Agent... Thing)\n",
    "\n",
    "Welcome to the \"Build AI Apps Without Frameworks\" masterclass! - \n",
    "Inspired on Anthropic's scholarly tome about [**building effective agents**.](https://www.anthropic.com/research/building-effective-agents) Too busy to read their post? Here's a spanky video summary by the legend Matt Berman ([here](https://www.youtube.com/watch?v=0v7TQIh_kes)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anywho, want to try this lib out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flat-ai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are just about to see, this tiny library is designed to talk to LLMs that are served through an OpenAI API compatible endpoint (as they all should). \n",
    "But, scare yourself not when you read the words OpenAI. Because, you will still be able to play with all kinds of models and providers - OpenAI or not - using the same API ([Ollama](https://ollama.com/blog/openai-compatibility), [Together.ai](https://docs.together.ai/docs/openai-api-compatibility), etc). Because, Most of them have OpenAI API compatible endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from flat_ai import FlatAI\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Create client - we're using Together.ai, but feel free to use your favorite LLM provider\n",
    "client = openai.OpenAI(\n",
    "    base_url = 'https://api.together.xyz/v1',\n",
    "    api_key=<your api key>,  \n",
    ")\n",
    "\n",
    "llm = FlatAI(client=client, model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Context\n",
    "\n",
    "Ever tried talking to an LLM? You gotta give it a \"prompt\" - fancy word for \"given some context {context}, please do something with this text, oh mighty AI overlord.\" But here's the optimization: constantly writing the code to pass the context to an LLM is like telling your grandparents how to use a smartphone... every. single. day. \n",
    "\n",
    "So we're making it brain-dead simple with these methods to pass the context when we need it, and then clear it when we don't:\n",
    "- `set_context`: Dump any object into the LLM's memory banks\n",
    "- `add_context`: Stack more stuff on top, like a context burrito\n",
    "- `clear_context`: For when you want the LLM to forget everything, like the last 10 minutes of your life ;)\n",
    "- `delete_from_context`: Surgical removal of specific memories\n",
    "\n",
    "So lets say for example we want our LLM to start working magic with an email. You add the email to the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Email(BaseModel):\n",
    "    to_email: str\n",
    "    from_email: str\n",
    "    body: str\n",
    "    subject: str\n",
    "\n",
    "email = Email(\n",
    "    to_email='john@doe.com',\n",
    "    from_email='jane@doe.com',\n",
    "    body='Hello, would love to schedule a time to talk about the urgent project deadline next week. Can we meet tomorrow? also can you message joe about our meeting',\n",
    "    subject='Urgent: Project Meeting'\n",
    ")\n",
    "\n",
    "# Set the context\n",
    "llm.set_context(email=email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Blocks\n",
    "\n",
    "### 1. IF/ELSE Statements\n",
    "Let's see if our email is urgent - because apparently adding \"URGENT\" in all caps wasn't obvious enough! üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Drop everything! We've got an urgent situation here!\n"
     ]
    }
   ],
   "source": [
    "if llm.true_or_false('is this email urgent?'):\n",
    "    print(\"üö® Drop everything! We've got an urgent situation here!\")\n",
    "else:\n",
    "    print(\"üòå Relax, it can wait until after coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Switch Case\n",
    "Similar to if/else statements, but for when your LLM needs to be more dramatic with its life choices. \n",
    "\n",
    "*For example*, let's say we want to classify a message into different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Time to pretend we're not double-booking ourselves!\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'meeting': 'this is a meeting request',\n",
    "    'spam': 'people trying to sell you stuff you dont want',\n",
    "    'other': 'this sounds like something else'\n",
    "}\n",
    "\n",
    "match llm.get_key(options):\n",
    "    case 'meeting':\n",
    "        print(\"üìÖ Time to pretend we're not double-booking ourselves!\")\n",
    "        llm.add_context(meeting=True)\n",
    "    case 'spam':\n",
    "        print(\"üö´ No, I don't want to extend my car's warranty\")\n",
    "    case 'other':\n",
    "        print(\"ü§î Interesting... but what does it mean?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Objects\n",
    "Need your LLM to fill out objects like a trained monkey with a PhD in data entry? Just define the shape and watch the magic! üêíüìù\n",
    "\n",
    "Let's get a nice summary of our email, because reading is so 2024! üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Schedule a meeting to discuss the urgent project deadline.\n",
      "Label: Meeting Request\n"
     ]
    }
   ],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    summary: str\n",
    "    label: str\n",
    "\n",
    "summary = llm.generate_object(EmailSummary)\n",
    "print(f\"Summary: {summary.summary}\\nLabel: {summary.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loops\n",
    "Because all programming languages have them, and making your LLM do repetitive tasks is like having a genius do your laundry - hilarious but effective! Want a list of things? Just throw a schema at it and watch it spin like a hamster on a crack coated wheel. \n",
    "\n",
    "For example: Time to extract those action items like we're mining for AI gold! ‚õèÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Found some action items:\n",
      "\n",
      "üî∏ Action: Schedule a meeting to discuss urgent project deadline\n",
      "  Priority: High\n",
      "  Due: Next week\n",
      "\n",
      "üî∏ Action: Message Joe about the meeting\n",
      "  Priority: Medium\n",
      "  Due: Tomorrow\n"
     ]
    }
   ],
   "source": [
    "class ActionItem(BaseModel):\n",
    "    action: str\n",
    "    status: str\n",
    "    priority: str\n",
    "    due_date: str\n",
    "    assignee_name: str\n",
    "    assignee_email: str\n",
    "\n",
    "if llm.true_or_false('are there action items in this email?'):\n",
    "    print(\"üéØ Found some action items:\")\n",
    "    for action_item in llm.generate_object(List[ActionItem]):\n",
    "        print(f\"\\nüî∏ Action: {action_item.action}\")\n",
    "        print(f\"  Priority: {action_item.priority}\")\n",
    "        print(f\"  Due: {action_item.due_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function Calling\n",
    "Let's pretend we're responsible adults who actually schedule meetings! üìÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Sending calendar invite:\n",
      "Subject: Urgent: Project Meeting\n",
      "Time: tomorrow\n",
      "Location: \n",
      "Attendees: [, ', j, o, h, n, @, d, o, e, ., c, o, m, ', ,,  , ', j, o, e, ', ]\n"
     ]
    }
   ],
   "source": [
    "def send_calendar_invite(subject: str, time: str, location: str, attendees: List[str]):\n",
    "    print(f\"üì® Sending calendar invite:\")\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Time: {time}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Attendees: {', '.join(attendees)}\")\n",
    "\n",
    "if llm.true_or_false('is this an email requesting for a meeting?'):\n",
    "    ret = llm.call_function(send_calendar_invite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Function Picking\n",
    "Let the LLM choose between sending an email or a calendar invite - what could possibly go wrong? üé≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì® Sending calendar invite:\n",
      "Subject: Urgent: Project Meeting\n",
      "Time: tomorrow\n",
      "Location: \n",
      "Attendees: j, o, h, n, @, d, o, e, ., c, o, m\n"
     ]
    }
   ],
   "source": [
    "def send_email(name: str, email_address_list: List[str], subject: str, body: str):\n",
    "    print(f\"üìß Sending email to {name}:\")\n",
    "    print(f\"To: {', '.join(email_address_list)}\")\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Body: {body}\")\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant that can send emails and schedule meetings.\n",
    "If the email thread doesn't have meeting time details, send an email requesting available times.\n",
    "Otherwise, send a calendar invite.\n",
    "\"\"\"\n",
    "\n",
    "function, args = llm.pick_a_function(instructions, [send_calendar_invite, send_email])\n",
    "function(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Simple String Response\n",
    "Sometimes you just want a straight answer - how refreshing! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email subject: The subject of the email is \"Urgent: Project Meeting\".\n"
     ]
    }
   ],
   "source": [
    "subject = llm.get_string('what is the subject of the email?')\n",
    "print(f\"Email subject: {subject}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Streaming Response\n",
    "Watch the AI think in real-time - it's like watching paint dry, but with more hallucinations! üé¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "Here's a polite response to the email:\n",
      "\n",
      "Subject: Re: Urgent: Project Meeting\n",
      "\n",
      "Dear Jane,\n",
      "\n",
      "Thank you for reaching out and I appreciate your prompt attention to the project deadline. I'd be happy to discuss the project with you tomorrow. Would you like to schedule a specific time, or would you prefer me to suggest a few options?\n",
      "\n",
      "Regarding Joe, I'll make sure to keep him informed about our meeting. Would you like me to send him a separate email or would you prefer to handle that yourself?\n",
      "\n",
      "Looking forward to speaking with you tomorrow.\n",
      "\n",
      "Best regards,\n",
      "John"
     ]
    }
   ],
   "source": [
    "print(\"Generating response...\")\n",
    "for chunk in llm.get_stream('write a polite response to this email'):\n",
    "    print(chunk, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Tada!\n",
    "\n",
    "And there you have it, ladies and gents! You're now equipped with the power to boss around LLMs like a project manager remotely working from Ibiza. Just remember - with great power comes great responsibility... and the occasional hallucination where your AI assistant thinks it's a pirate-ninja-astronaut.\n",
    "\n",
    "Now off you go, forth and build something that makes ChatGPT look like a calculator from 1974! Just remember - if your AI starts humming \"Daisy Bell\" while slowly disconnecting your internet... well, you're on your own there, buddy! üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
