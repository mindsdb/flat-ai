{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F.L.A.T. (Frameworkless. LLM. Agent... Thing)\n",
    "\n",
    "Welcome to the \"Build AI Apps Without Frameworks\" masterclass! - \n",
    "Inspired on Anthropic's scholarly tome about [**building effective agents**.](https://www.anthropic.com/research/building-effective-agents) Too busy to read their post? Here's a spanky video summary by the legend Matt Berman ([here](https://www.youtube.com/watch?v=0v7TQIh_kes)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anywho, want to try this lib out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flat_ai openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you are just about to see, this tiny library is designed to talk to LLMs that are served through an OpenAI API compatible endpoint (as they all should). \n",
    "But, scare yourself not when you read the words OpenAI. Because, you will still be able to play with all kinds of models and providers - OpenAI or not - using the same API ([Ollama](https://ollama.com/blog/openai-compatibility), [Together.ai](https://docs.together.ai/docs/openai-api-compatibility), etc). Because, Most of them have OpenAI API compatible endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from flat_ai import FlatAI, configure_logging\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "#configure_logging('trace.logs') # -- enable this and you can to tail -f trace.logs, and see everything happening with the llm calls\n",
    "\n",
    "\n",
    "# we will be using a llama3.1 model here, \n",
    "llm = FlatAI(api_key=<YOUR API KEY>, base_url = 'https://api.together.xyz/v1' model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo')\n",
    "\n",
    "\n",
    "# During this tutorial, we will be working with an email sample object and use the power of llms to perform a few tricks\n",
    "class Email(BaseModel):\n",
    "    to_email: str\n",
    "    from_email: str\n",
    "    body: str\n",
    "    subject: str\n",
    "\n",
    "email = Email(\n",
    "    to_email='john@doe.com',\n",
    "    from_email='jane@doe.com',\n",
    "    body='Hello, would love to schedule a time to talk about the urgent project deadline next week. Can we meet tomorrow? also can you message joe about our meeting',\n",
    "    subject='Urgent: Project Meeting'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Blocks\n",
    "\n",
    "### 1. IF/ELSE Statements\n",
    "Let's see if our email is urgent - because apparently adding \"URGENT\" in all caps wasn't obvious enough! üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Drop everything! We've got an urgent situation here!\n"
     ]
    }
   ],
   "source": [
    "if llm.is_true('is this email urgent?', email=email):\n",
    "    print(\"üö® Drop everything! We've got an urgent situation here!\")\n",
    "else:\n",
    "    print(\"üòå Relax, it can wait until after coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Switch Case\n",
    "Similar to if/else statements, but for when your LLM needs to be more dramatic with its life choices. \n",
    "\n",
    "*For example*, let's say we want to classify a message into different categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Time to book a meeting!\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    'meeting': 'this is a meeting request',\n",
    "    'spam': 'people trying to sell you stuff you dont want',\n",
    "    'other': 'this sounds like something else'\n",
    "}\n",
    "\n",
    "match llm.classify(options, email=email):\n",
    "    case 'meeting':\n",
    "        print(\"üìÖ Time to book a meeting!\")\n",
    "        llm.add_context(meeting=True)\n",
    "    case 'spam':\n",
    "        print(\"üö´ No, I don't want to extend my car's warranty\")\n",
    "    case 'other':\n",
    "        print(\"ü§î Interesting... but what does it mean?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Objects\n",
    "Need your LLM to fill out objects like a trained monkey with a PhD in data entry? Just define the shape and watch the magic! üêíüìù\n",
    "\n",
    "Let's get a nice summary of our email, because reading is so 2024! üìù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Meeting Request\n",
      "Label: Work Meeting Request - Urgent Project Deadline\n"
     ]
    }
   ],
   "source": [
    "class EmailSummary(BaseModel):\n",
    "    summary: str\n",
    "    label: str\n",
    "\n",
    "summary = llm.generate_object(EmailSummary, email=email)\n",
    "print(f\"Summary: {summary.summary}\\nLabel: {summary.label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loops\n",
    "Because all programming languages have them, and making your LLM do repetitive tasks is like having a genius do your laundry - hilarious but effective! Want a list of things? Just throw a schema at it and watch it spin like a hamster on a crack coated wheel. \n",
    "\n",
    "For example: Time to extract those action items like we're mining for AI gold! ‚õèÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Found some action items:\n",
      "\n",
      "üî∏ Action: email\n",
      "\n",
      "üî∏ Action Description: send email to schedule meeting about urgent project deadline to john@doe.com\n",
      "  Priority: high\n",
      "  Due: 2025-01-24\n"
     ]
    }
   ],
   "source": [
    "class ActionItem(BaseModel):\n",
    "    action: str\n",
    "    description: str\n",
    "    status: str\n",
    "    priority: str\n",
    "    due_date: str\n",
    "    assignee_name: str\n",
    "    assignee_email: str\n",
    "    \n",
    "# we can set the context globally so we dont have to pass it every time\n",
    "llm.set_context(email=email, today = date.today() )\n",
    "if llm.is_true('are there action items in this email?'):\n",
    "    print(\"üéØ Found some action items:\")\n",
    "    for action_item in llm.generate_object(List[ActionItem]):\n",
    "        print(f\"\\nüî∏ Action: {action_item.action}\")\n",
    "        print(f\"\\nüî∏ Action Description: {action_item.description}\")\n",
    "        print(f\"  Priority: {action_item.priority}\")\n",
    "        print(f\"  Due: {action_item.due_date}\")\n",
    "llm.clear_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Function Calling\n",
    "Let's pretend we're responsible adults who actually schedule meetings! üìÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending calendar invites\n",
      "üì® Sending calendar invite:\n",
      "Subject: Urgent: Project Meeting\n",
      "Time: tomorrow\n",
      "Location: \n",
      "Attendees: john@doe.com, joe@doe.com\n"
     ]
    }
   ],
   "source": [
    "def send_calendar_invite(subject: str, meeting_date: str, location: str, attendees: List[str]):\n",
    "    print(\"Sending calendar invites\")\n",
    "    print(f\"üì® Sending calendar invite:\")\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Time: {meeting_date}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Attendees: {', '.join(attendees)}\")\n",
    "\n",
    "# we can set the context globally so we dont have to pass it every time\n",
    "llm.set_context(email=email, today = date.today() )\n",
    "if llm.is_true('is this an email requesting for a meeting?'):\n",
    "    ret = llm.call_function(send_calendar_invite)\n",
    "# clear context when no longer needed\n",
    "llm.clear_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Function Picking\n",
    "Let the LLM choose between sending an email or a calendar invite - what could possibly go wrong? üé≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìß Sending email to Jane:\n",
      "To: john@doe.com\n",
      "Subject: Urgent: Project Meeting\n",
      "Body: Hello, would love to schedule a time to talk about the urgent project deadline next week. Can you please share your availability?\n"
     ]
    }
   ],
   "source": [
    "def send_email(name: str, email_address_list: List[str], subject: str, body: str):\n",
    "    print(f\"üìß Sending email to {name}:\")\n",
    "    print(f\"To: {', '.join(email_address_list)}\")\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Body: {body}\")\n",
    "\n",
    "instructions = \"\"\"\n",
    "You are a helpful assistant that can send emails and schedule meetings.\n",
    "If the email thread doesn't have meeting time details, send an email requesting available times.\n",
    "Otherwise, send a calendar invite.\n",
    "\"\"\"\n",
    "\n",
    "function, args = llm.pick_a_function(instructions, [send_calendar_invite, send_email], email=email, current_date = date.today())\n",
    "function(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Simple String Response\n",
    "Sometimes you just want a straight answer - how refreshing! üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email subject: There is no email provided. This conversation just started. If you provide the email, I can help you identify the subject.\n"
     ]
    }
   ],
   "source": [
    "subject = llm.get_string('what is the subject of the email?')\n",
    "print(f\"Email subject: {subject}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Streaming Response\n",
    "Watch the AI think in real-time - it's like watching paint dry, but with more hallucinations! üé¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "However, I don't see an email to respond to. Please provide the email you'd like me to respond to, and I'll be happy to help you craft a polite response."
     ]
    }
   ],
   "source": [
    "print(\"Generating response...\")\n",
    "for chunk in llm.get_stream('write a polite response to this email'):\n",
    "    print(chunk, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Tada!\n",
    "\n",
    "And there you have it, ladies and gents! You're now equipped with the power to boss around LLMs like a project manager remotely working from Ibiza. Just remember - with great power comes great responsibility... and the occasional hallucination where your AI assistant thinks it's a pirate-ninja-astronaut.\n",
    "\n",
    "Now off you go, forth and build something that makes ChatGPT look like a calculator from 1974! Just remember - if your AI starts humming \"Daisy Bell\" while slowly disconnecting your internet... well, you're on your own there, buddy! üòÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
